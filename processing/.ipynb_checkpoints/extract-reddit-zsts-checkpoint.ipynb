{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves to extract reddit comment and post data from .zst files retrieved from the pushshift database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import zstandard as zstd\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "```\n",
    "with open(\"filename.zst\", 'rb') as fh:\n",
    "    dctx = zstd.ZstdDecompressor()\n",
    "    with dctx.stream_reader(fh) as reader:\n",
    "        previous_line = \"\"\n",
    "        while True:\n",
    "            chunk = reader.read(65536)\n",
    "            if not chunk:\n",
    "                break\n",
    "\n",
    "            string_data = chunk.decode('utf-8')\n",
    "            lines = string_data.split(\"\\n\")\n",
    "            for i, line in enumerate(lines[:-1]):\n",
    "                if i == 0:\n",
    "                    line = previous_line + line\n",
    "                object = json.loads(line)\n",
    "                # do something with the object here\n",
    "            previous_line = lines[-1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5364 posts extracted.\n"
     ]
    }
   ],
   "source": [
    "filename = \"../../data/tabular/reddit_post_dumps/RS_2019-09.zst\"\n",
    "wsb_posts = []\n",
    "with open(filename, 'rb') as fh:\n",
    "    dctx = zstd.ZstdDecompressor()\n",
    "    \n",
    "    with dctx.stream_reader(fh) as reader:\n",
    "        previous_line = \"\"\n",
    "        while True:\n",
    "            chunk = reader.read(2**24)\n",
    "            if not chunk:\n",
    "                break\n",
    "            string_data = chunk.decode('utf-8')\n",
    "            lines = string_data.split(\"\\n\")\n",
    "            for i, line in enumerate(lines[:-1]):\n",
    "                if i == 0:\n",
    "                    line = previous_line + line\n",
    "                post = json.loads(line)\n",
    "                if post['subreddit'].lower() == 'wallstreetbets':\n",
    "                    wsb_posts.append(post)\n",
    "            previous_line = lines[-1]\n",
    "            \n",
    "print('{} posts extracted.'.format(len(wsb_posts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 5052 from RS_2019-10.zst\n",
      "No subreddit errors detected.\n",
      "Extracted 7604 from RS_2019-11.zst\n",
      "No subreddit errors detected.\n",
      "Extracted 6110 from RS_2019-12.zst\n",
      "No subreddit errors detected.\n"
     ]
    }
   ],
   "source": [
    "prefix = \"../../data/tabular/reddit_post_dumps\"\n",
    "filenames = [\"RS_2019-10\", \"RS_2019-11\", \"RS_2019-12\"]\n",
    "\n",
    "for filename in filenames:\n",
    "    wsb_posts = []\n",
    "    \n",
    "    with open('{}/{}.zst'.format(prefix, filename), 'rb') as fh:\n",
    "        dctx = zstd.ZstdDecompressor()\n",
    "        with dctx.stream_reader(fh) as reader:\n",
    "            previous_line = \"\"\n",
    "            while True:\n",
    "                chunk = reader.read(2**24)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                string_data = chunk.decode('utf-8')\n",
    "                lines = string_data.split(\"\\n\")\n",
    "                for i, line in enumerate(lines[:-1]):\n",
    "                    if i == 0:\n",
    "                        line = previous_line + line\n",
    "                    post = json.loads(line)\n",
    "                    if post['subreddit'].lower() == 'wallstreetbets':\n",
    "                        wsb_posts.append(post)\n",
    "                previous_line = lines[-1]\n",
    "    \n",
    "    with open('{}/{}.json'.format(prefix, filename), 'w') as json_file:\n",
    "        json.dump(wsb_posts, json_file)\n",
    "        \n",
    "    print('Extracted {} from {}.zst'.format(len(wsb_posts), filename))\n",
    "    \n",
    "    if all([p['subreddit'] == 'wallstreetbets' for p in wsb_posts]):\n",
    "        print('No subreddit errors detected.')\n",
    "    else:\n",
    "        print('Subreddit contamination detected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all([p['subreddit'] == 'wallstreetbets' for p in test_load])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open('../../data/tabular/reddit_post_dumps/RS_2019-09.json', 'w') as json_file:\n",
    "    wsb_2019_09 = json.dump(wsb_posts, json_file) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py37)",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
