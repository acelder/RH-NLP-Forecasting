{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process Text Data\n",
    "\n",
    "This notebook will convert the reddit post text to model-readable features, perform train-test splits, and save to .csv files. Applies varying lags to processed text data for different prediction horizons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found, loading...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>tickers</th>\n",
       "      <th>num_tickers</th>\n",
       "      <th>text</th>\n",
       "      <th>mentions_option</th>\n",
       "      <th>mentions_call</th>\n",
       "      <th>mentions_put</th>\n",
       "      <th>text_hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-01 00:06:43</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>QQQ</td>\n",
       "      <td>1</td>\n",
       "      <td>What kind of option play would you use to prof...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150508729389721599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-05-01 00:12:41</td>\n",
       "      <td>12</td>\n",
       "      <td>41</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1</td>\n",
       "      <td>Tariffs are delayed meaning AAPL calls are gua...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10678551544699203972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1          created_utc  num_comments  score tickers  \\\n",
       "0           0             0  2018-05-01 00:06:43            24      3     QQQ   \n",
       "1           1             1  2018-05-01 00:12:41            12     41    AAPL   \n",
       "\n",
       "   num_tickers                                               text  \\\n",
       "0            1  What kind of option play would you use to prof...   \n",
       "1            1  Tariffs are delayed meaning AAPL calls are gua...   \n",
       "\n",
       "   mentions_option  mentions_call  mentions_put             text_hash  \n",
       "0                1              0             0    150508729389721599  \n",
       "1                0              1             0  10678551544699203972  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "exp_file = '../data/tabular/wsb_posts/cleaned posts.csv'\n",
    "\n",
    "if os.path.isfile(exp_file):\n",
    "    print('File found, loading...')\n",
    "    combined_df = pd.read_csv(exp_file)\n",
    "\n",
    "else:\n",
    "    print('File not found, processing data from files...')\n",
    "    \n",
    "    def load_posts1(file):\n",
    "        df = pd.read_csv(file)\n",
    "        df['created_utc'] = pd.to_datetime(df['created_utc'],\n",
    "                                           origin = 'unix',\n",
    "                                           unit = 's')\n",
    "        df.loc[df['selftext'].isna(), 'selftext'] = ''\n",
    "        df = df.sort_values('created_utc')\n",
    "        return df[['created_utc', 'num_comments', 'score', 'title', 'selftext']]\n",
    "    posts_2018 = load_posts1('../data/tabular/wsb_posts/WSB Posts 2018.csv')\n",
    "\n",
    "\n",
    "\n",
    "    def extract_wrapper(pattern):\n",
    "        def extractor(string):\n",
    "            prog = re.compile(pattern)\n",
    "            results = [list(elem) for elem in prog.findall(string)]\n",
    "            if len(results) == 0:\n",
    "                return results\n",
    "            elif len(results) == 1:\n",
    "                return [elem for elem in results[0] if elem.isalpha()]\n",
    "            else:\n",
    "                result = results[0]\n",
    "                for next_res in results[1:]:\n",
    "                    result = [elem for elem in result if elem.isalpha()]\n",
    "                    result += next_res\n",
    "                return result\n",
    "        return extractor\n",
    "\n",
    "    # Extact tickers from text\n",
    "    def extract_tickers(df):\n",
    "        df = df.copy()\n",
    "        df['selftext'] = df['selftext'].fillna('')\n",
    "        patterns = [\n",
    "            r'\\$([A-Z]{1}) +',\n",
    "            r'\\$([A-Z]{2})(\\s|[0-9a-z]|\\Z|\\W)',\n",
    "            r'\\$([A-Z]{3})(\\s|[0-9a-z]|\\Z|\\W)',\n",
    "            r'\\$([A-Z]{4})(\\s|[0-9a-z]|\\Z|\\W)',\n",
    "            r'(\\A|\\s)([A-Z]{1}) +',\n",
    "            r'(\\A|\\s)([A-Z]{2})(\\s|[0-9a-z]|\\Z|\\W)',\n",
    "            r'(\\A|\\s)([A-Z]{3})(\\s|[0-9a-z]|\\Z|\\W)',\n",
    "            r'(\\A|\\s)([A-Z]{4})(\\s|[0-9a-z]|\\Z|\\W)'\n",
    "        ]\n",
    "\n",
    "        results = [df['title'].apply(extract_wrapper(pattern)) for pattern in patterns]\n",
    "        results = results + [df['selftext'].apply(extract_wrapper(pattern)) for pattern in patterns]\n",
    "\n",
    "        new_col = results[0]\n",
    "\n",
    "        for result in results[1:]:\n",
    "            new_col += result\n",
    "\n",
    "        df['tickers'] = new_col\n",
    "        df['num_tickers'] = df['tickers'].apply(len)\n",
    "        return df\n",
    "\n",
    "\n",
    "    extracted_2018 = extract_tickers(posts_2018)\n",
    "    extracted_2018 = extracted_2018[extracted_2018.num_tickers > 0]\n",
    "\n",
    "    # validate tickers\n",
    "    valid_tickers = set(pd.read_csv('../data/tabular/tickers.csv').Ticker.values)\n",
    "    valid_tickers = valid_tickers.difference({'YOLO', 'IPO'})  # Remove YOLO ticker\n",
    "\n",
    "    def validate_tickers(df, valid_set):\n",
    "        df = df.copy()\n",
    "        df['tickers'] = df['tickers'].apply(\n",
    "            lambda x: list(set([\n",
    "                ticker for ticker in x if ticker in valid_set\n",
    "            ]))\n",
    "        )\n",
    "        df['num_tickers'] = df['tickers'].apply(len)\n",
    "        return df\n",
    "\n",
    "    validated_2018 = validate_tickers(extracted_2018, valid_tickers)\n",
    "    validated_2018 = validated_2018[validated_2018.num_tickers > 0]\n",
    "\n",
    "    # Combine text columns\n",
    "    def reduce_columns(df):\n",
    "        df = df.copy()\n",
    "        df['text'] = df['title']  + '\\n' + df['selftext']\n",
    "        df = df.drop(columns=['title', 'selftext'])\n",
    "        return df\n",
    "\n",
    "    reduced_2018 = reduce_columns(validated_2018)\n",
    "\n",
    "\n",
    "    posts_2019 = load_posts1('../data/tabular/wsb_posts/WSB Posts 2019.csv')\n",
    "    extracted_2019 = extract_tickers(posts_2019)\n",
    "    extracted_2019 = extracted_2019[extracted_2019.num_tickers > 0]\n",
    "    validated_2019 = validate_tickers(extracted_2019, valid_tickers)\n",
    "    validated_2019 = validated_2019[validated_2019.num_tickers > 0]\n",
    "    reduced_2019 = reduce_columns(validated_2019)\n",
    "\n",
    "    def load_posts2():\n",
    "        prefix = '../data/heirarchical/wsb posts 2019'\n",
    "        filenames  = [\n",
    "            'RS_2019-09.json',\n",
    "            'RS_2019-10.json',\n",
    "            'RS_2019-11.json',\n",
    "            'RS_2019-12.json'\n",
    "        ]\n",
    "        df = pd.read_json('{}/{}'.format(prefix, filenames[0]))\n",
    "        for name in filenames[1:]:\n",
    "            df = pd.concat((df, pd.read_json('{}/{}'.format(prefix, name))))\n",
    "        df['created_utc'] = pd.to_datetime(df['created_utc'],\n",
    "                                           origin = 'unix',\n",
    "                                           unit = 's')\n",
    "        df.loc[df['selftext'].isna(), 'selftext'] = ''\n",
    "        df = df.sort_values('created_utc')\n",
    "        return df[['created_utc', 'num_comments', 'score', 'title', 'selftext']]\n",
    "\n",
    "    remaining_2019 = load_posts2()\n",
    "\n",
    "    extracted_remaining = extract_tickers(remaining_2019)\n",
    "    extracted_remaining = extracted_remaining[extracted_remaining.num_tickers > 0]\n",
    "    validated_remaining = validate_tickers(extracted_remaining, valid_tickers)\n",
    "    validated_remaining = validated_remaining[validated_remaining.num_tickers > 0]\n",
    "    reduced_remaining = reduce_columns(validated_remaining)\n",
    "\n",
    "\n",
    "    combined_df = pd.concat((reduced_2018, reduced_2019, reduced_remaining))\n",
    "    combined_df.index = np.arange(combined_df.shape[0])\n",
    "    combined_df['tickers'] = combined_df.tickers.apply(lambda x: ','.join(x))\n",
    "\n",
    "    # let's add a couple columns for words that we expect to be important\n",
    "    combined_df['mentions_option'] = combined_df['text'].apply(lambda x: 'option' in x.lower()).astype(int)\n",
    "    combined_df['mentions_call'] = combined_df['text'].apply(lambda x: 'call' in x.lower()).astype(int)\n",
    "    combined_df['mentions_put'] = combined_df['text'].apply(lambda x: 'put' in x.lower()).astype(int)\n",
    "\n",
    "    # finally, add a hash for the text string so that we can quickly relate data to it later\n",
    "    combined_df['text_hash'] = pd.util.hash_pandas_object(combined_df['text'])\n",
    "\n",
    "    combined_df.to_csv(exp_file)\n",
    "\n",
    "combined_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some samples of post text to get some intuition for the documents in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would this work?\n",
      "Letâ€™s say I buy 1,000 shares in Company A for $2 per share ($2,000 total). I then negotiate a futures contract where I buy more shares in Company A, but for $50 a share. Because the stock price is dependent on what it was last traded at, if I got a shit load of futures, where I buy stock at massively inflated prices, would this then cause the stock price to rise significantly? Then I can sell my original investment for $50,000. \n",
      "\n",
      "Or am I just being retarded?\n",
      "-----------\n",
      "BUY $JNUG IT WILL SKYROCKET SHORT SQUEEZE\n",
      "[removed]\n",
      "-----------\n",
      "I feel like I am cheating on my computer by investing in AMD. Intel gang.\n",
      "[deleted]\n",
      "-----------\n",
      "PSA: The funding is NOT secured.\n",
      "[deleted]\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "samples = combined_df.text.sample(4)\n",
    "for _, sample in samples.iteritems():\n",
    "    print(sample)\n",
    "    print('-----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand Tickers Column\n",
    "\n",
    "Expand the tickers column and keep the text_hash column to relate back to the compressed comment dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found, loading...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ticker</th>\n",
       "      <th>text_hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>QQQ</td>\n",
       "      <td>150508729389721599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>10678551544699203972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I</td>\n",
       "      <td>10730372757830112337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>FB</td>\n",
       "      <td>10730372757830112337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>DD</td>\n",
       "      <td>10730372757830112337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 ticker             text_hash\n",
       "0           0    QQQ    150508729389721599\n",
       "1           1   AAPL  10678551544699203972\n",
       "2           2      I  10730372757830112337\n",
       "3           3     FB  10730372757830112337\n",
       "4           4     DD  10730372757830112337"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expand combined_df so that each row has only one ticker\n",
    "\n",
    "exp_file = '../../data/tabular/wsb_posts/expanded cleaned posts.csv'\n",
    "\n",
    "if os.path.isfile(exp_file):\n",
    "    print('File found, loading...')\n",
    "    expanded_df = pd.read_csv(exp_file)\n",
    "    \n",
    "\n",
    "else:\n",
    "    print('File not found, expanding dataframe...')\n",
    "    expanded_df = pd.DataFrame(columns = ['ticker', 'text_hash'])\n",
    "\n",
    "    for i, row in combined_df.iterrows():\n",
    "\n",
    "        row_tickers = row.tickers.split(',')\n",
    "        new_rows = pd.DataFrame({'ticker': row_tickers,\n",
    "                                 'text_hash': row.text_hash})\n",
    "        expanded_df = pd.concat((expanded_df, new_rows))\n",
    "\n",
    "    expanded_df.index = np.arange(expanded_df.shape[0])\n",
    "    expanded_df.to_csv(exp_file)\n",
    "    \n",
    "expanded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "\n",
    "Perform a train/test split on the text data using the combined_df table of wsb comments. We're interested in data drift over time so we'll reserve the test data to the last 5% of comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = pd.to_datetime('2019-10-01')\n",
    "\n",
    "train_df  = combined_df[pd.to_datetime(combined_df['created_utc']) < split_date]\n",
    "test_df   = combined_df[pd.to_datetime(combined_df['created_utc']) >= split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kind', 'option', 'play', 'would', 'use', 'profit', 'go', 'past']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "def process_text(string):\n",
    "    # Approach is drawn from sentiment analysis project, skips\n",
    "    # the HTML processing as it's not needed here.\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    words   = [w.strip() for w in string.lower().split() if w not in stopwords.words(\"english\")]\n",
    "    words   = [w for w in words if w.isalpha()]\n",
    "    words   = [stemmer.stem(w) for w in words]\n",
    "    \n",
    "    return words\n",
    "\n",
    "process_text(train_df.iloc[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply text processing to each row of the train and test sets and\n",
    "# store as dicts\n",
    "\n",
    "train_X_file = '../../data/tabular/processed data/compressed_train_X_df.csv'\n",
    "test_X_file = '../../data/tabular/processed data/compressed_test_X_df.csv'\n",
    "\n",
    "if os.path.isfile(train_X_file) and os.path.isfile(test_X_file):\n",
    "    train_X_df = pd.read_csv(train_X_file, usecols=['text_hash', 'text'])\n",
    "    test_X_df = pd.read_csv(test_X_file, usecols=['text_hash', 'text'])\n",
    "else:\n",
    "    train_X = {row.text_hash: \" \".join(process_text(row.text)) for idx, row in train_df.iterrows()}\n",
    "    test_X = {row.text_hash: \" \".join(process_text(row.text)) for idx, row in test_df.iterrows()}\n",
    "\n",
    "    train_X_df = pd.DataFrame([[key, item] for key, item in train_X.items()],\n",
    "                              columns = ['text_hash', 'text'])\n",
    "    test_X_df = pd.DataFrame([[key, item] for key, item in test_X.items()],\n",
    "                             columns = ['text_hash', 'text'])\n",
    "    train_X_df.to_csv(\"../../data/tabular/processed data/compressed_train_X_df.csv\")\n",
    "    test_X_df.to_csv(\"../../data/tabular/processed data/compressed_test_X_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "vectorizers = {\n",
    "    'count_word': CountVectorizer(max_features=1000),\n",
    "    'count_bigram': CountVectorizer(analyzer='word', ngram_range=(2, 2), max_features=1000),\n",
    "    'tfidf_word': TfidfVectorizer(max_features=1000),\n",
    "    'tfidf_bigram': TfidfVectorizer(analyzer='word', ngram_range=(2, 2), max_features=1000)\n",
    "}\n",
    "\n",
    "vectorizers = {key: vec.fit(train_X_df['text'].fillna('')) for key, vec in vectorizers.items()}\n",
    "\n",
    "\n",
    "X_train_vec, X_test_vec = (\n",
    "    {\n",
    "        key: pd.concat((\n",
    "            pd.DataFrame(vect.transform(x_df['text'].fillna('')).toarray(),\n",
    "                         columns = vect.get_feature_names()),\n",
    "            x_df['text_hash']\n",
    "        ), axis=1)\n",
    "        for key, vect in vectorizers.items()\n",
    "    }\n",
    "    for x_df in [train_X_df, test_X_df]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizers.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save vectorized data\n",
    "for key in vectorizers.keys():\n",
    "    X_train_vec[key].to_csv('../../data/tabular/processed data/{}_train_X.csv'.format(key))\n",
    "    X_test_vec[key].to_csv('../../data/tabular/processed data/{}_test_X.csv'.format(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transition_df = pd.read_csv('../../data/tabular/90th percentile transitions.csv',\n",
    "                            usecols=['Date', 'Ticker', 'Transition'])\n",
    "full_df = combined_df.copy()\n",
    "full_df['created_utc'] = pd.to_datetime(full_df['created_utc'])\n",
    "full_df['Date'] = pd.to_datetime(full_df['created_utc'].dt.date)\n",
    "full_df = pd.merge(full_df, expanded_df, how='right', on='text_hash')\n",
    "\n",
    "transition_df['Date'] = pd.to_datetime(transition_df['Date'])\n",
    "\n",
    "full_df = (\n",
    "    pd.merge(full_df, \n",
    "             transition_df, \n",
    "             left_on = ['Date', 'ticker'], \n",
    "             right_on = ['Date', 'Ticker'],\n",
    "             how = 'left')\n",
    "    .drop(columns = [col for col in full_df.columns if 'Unnamed' in col])\n",
    ")\n",
    "\n",
    "\n",
    "def apply_lag(df, lag):\n",
    "\n",
    "    new_col = 'TransitionAfterLag'\n",
    "    \n",
    "    # Subtract a day from Date column to avoid information leaks\n",
    "    df['Date'] = df['Date'] - pd.to_timedelta(1, unit='d')\n",
    "    \n",
    "    return (\n",
    "        df\n",
    "        .pivot(index='Date', columns=['Ticker'])\n",
    "        .sort_index(ascending=False)\n",
    "        .rolling(window=5)\n",
    "        .max()\n",
    "        .sort_index(ascending=True)\n",
    "        .dropna()\n",
    "        .reset_index('Date')\n",
    "        .melt(id_vars='Date', value_name=new_col)\n",
    "        [['Date', 'Ticker', new_col]]\n",
    "    )\n",
    "    \n",
    "lag_transition_dfs = {lag: apply_lag(transition_df, lag) for lag in [3, 5, 10]}\n",
    "\n",
    "lag_full_dfs = {\n",
    "    lag: (\n",
    "        pd.merge(full_df,\n",
    "                 df,\n",
    "                 left_on = ['Date', 'ticker'], \n",
    "                 right_on = ['Date', 'Ticker'],\n",
    "                 how = 'left')\n",
    "        .drop(columns = [col for col in full_df.columns if 'Unnmeed' in col])\n",
    "    )\n",
    "    for lag, df in lag_transition_dfs.items()\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lag dfs\n",
    "for lag in [3, 5, 10]:\n",
    "    filename = '../../data/tabular/processed data/transition_map_lag_{}.csv'.format(lag)\n",
    "    lag_full_dfs[lag][['text_hash', 'TransitionAfterLag']].to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of ticker-date combinations corresponding to transitions: 0.37%\n"
     ]
    }
   ],
   "source": [
    "transitions = (\n",
    "        full_df\n",
    "        [['Ticker', 'Date', 'Transition']]\n",
    "        [full_df['Transition'] != 0]\n",
    "        .groupby(['Date', 'Ticker'])\n",
    "        .count()\n",
    "    )\n",
    "non_transitions = (\n",
    "    full_df\n",
    "    [['Ticker', 'Date']]\n",
    "    [full_df['Transition'] == 0]\n",
    "    .groupby(['Date', 'Ticker'])\n",
    "    .count()\n",
    ")\n",
    "pct_trans = transitions.shape[0]/non_transitions.shape[0]\n",
    "print('Percent of ticker-date combinations corresponding to transitions: {:.2%}'.format(pct_trans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_examples(sample_period, \n",
    "                    n_target, \n",
    "                    n_random, \n",
    "                    text_df,\n",
    "                    n_examples,\n",
    "                    trans_freq=None,\n",
    "                    verbose=False):\n",
    "    \"\"\"\n",
    "    Create examples for neural net or bag-of-words models by sampling comments mentioning the\n",
    "    target stock and comments mentioning other random stocks in the days leading up to a transition of\n",
    "    the target into the top 10% of accounts holding. The idea is to allow the model to compare attitudes\n",
    "    toward the target name with the broader market. Generates the population statistically.\n",
    "    \n",
    "    TODO: Test examples mode samples all transitions from the period.\n",
    "    TODO: An earlier join causes comments to only be sampled from trading days. Modify such that \n",
    "          comments on weekends and holidays can also be used.\n",
    "    \n",
    "    @param sample_period number of days leading up to the transition date from which to\n",
    "    sample comments. Does not include the date of transition. \n",
    "    @param n_target the number of comments pertaining to the target that will be included in the nn\n",
    "    input\n",
    "    @param n_random the number of comments to random stocks (no including the target) that will\n",
    "    be included in the input\n",
    "    @param text_df DataFrame containing comment text, associated tickers, and indicator of transition day\n",
    "    @param verbose Boolean Indicates if the procedure will print progress and warnings to the command line.\n",
    "    \"\"\"\n",
    "    trans_ct = (\n",
    "        text_df\n",
    "        [['Ticker', 'Date', 'Transition']]\n",
    "        [text_df['Transition'] != 0]\n",
    "        .groupby(['Date', 'Ticker'])\n",
    "        .count()\n",
    "    )\n",
    "    non_trans_ct = (\n",
    "        text_df\n",
    "        [['Ticker', 'Date', 'Transition']]\n",
    "        [text_df['Transition'] == 0]\n",
    "        .groupby(['Date', 'Ticker'])\n",
    "        .count()\n",
    "    )\n",
    "\n",
    "    examples = []\n",
    "    \n",
    "    if trans_freq is None:\n",
    "        trans_freq = trans_ct.shape/non_trans_ct.shape\n",
    "        \n",
    "    for i in range(n_examples):\n",
    "        select_trans_ct = np.random.rand() <= trans_freq\n",
    "        \n",
    "        target_pair = np.random.choice(trans_ct.index) if select_trans_ct else np.random.choice(non_trans_ct.index)\n",
    "        # sample n_target from preceding sample period\n",
    "        preceding_target = text_df[((text_df['created_utc'] < target_pair[0]) &\n",
    "                                   (text_df['created_utc'] > target_pair[0] - pd.to_timedelta(sample_period, unit='d')) &\n",
    "                                   (text_df['Ticker'] == target_pair[1]))]\n",
    "        \n",
    "        if preceding_target.shape[0] < n_target:\n",
    "            if verbose:\n",
    "                print('WARNING: Not enough target samples available for transition target, proceeding to next sample.')\n",
    "            continue\n",
    "            \n",
    "        target_samples = np.random.choice(preceding_target['text_hash'])\n",
    "        # sample n_random from preceding sample period\n",
    "        \n",
    "        preceding_random = text_df[(text_df['created_utc'] < target_pair[0]) &\n",
    "                                   (text_df['created_utc'] >= target_pair[0] - pd.to_timedelta(sample_period, unit='d')) &\n",
    "                                   ~(text_df['Ticker'] == target_pair[1])]\n",
    "        if preceding_target.shape[0] <= n_random:\n",
    "            if verbose:\n",
    "                print('WARNING: Not enough random samples available for transition target, proceeding to next sample.')\n",
    "            continue\n",
    "        \n",
    "        random_samples = np.random.choice(preceding_random['text_hash'], n_random)\n",
    "        # append data\n",
    "        example = {\n",
    "            'transition': select_trans_ct,\n",
    "            'date': str(target_pair[0]),\n",
    "            'target': target_pair[1],\n",
    "            'target_samples': target_samples,\n",
    "            'random_samples': list(random_samples)\n",
    "        }\n",
    "        examples.append(example)\n",
    "    return examples\n",
    "        \n",
    "\n",
    "\n",
    "sample_test_df = full_df[full_df['created_utc'] <= pd.to_datetime('2018-05-18')].copy()\n",
    "test_inputs = create_examples(sample_period=5, \n",
    "                              n_target=1,\n",
    "                              n_random=1,\n",
    "                              text_df=sample_test_df, \n",
    "                              n_examples=5,\n",
    "                              trans_freq=1.)\n",
    "\n",
    "assert test_inputs[0]['target'] in sample_test_df.loc[sample_test_df['text_hash'] == test_inputs[0]['target_samples'], 'Ticker'].values\n",
    "assert test_inputs[0]['target'] not in sample_test_df.loc[sample_test_df['text_hash'] == test_inputs[0]['random_samples'][0], 'Ticker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train/test examples for 3 trailing days: 785/153\n",
      "Number of train/test examples for 5 trailing days: 1046/195\n",
      "Number of train/test examples for 10 trailing days: 1404/291\n"
     ]
    }
   ],
   "source": [
    "# separate train and test temporally to avoid information leaks\n",
    "split_date = pd.to_datetime('2019-10-01')\n",
    "\n",
    "train_period_df = full_df[full_df['created_utc'] < split_date]\n",
    "test_period_df = full_df[full_df['created_utc'] >= split_date]\n",
    "\n",
    "# examine sample periods of 3, 5, and 10 days\n",
    "train_examples = {\n",
    "    sample_period: create_examples(sample_period=sample_period,\n",
    "                                   n_target=1,\n",
    "                                   n_random=3,\n",
    "                                   text_df=train_period_df,\n",
    "                                   n_examples=5000,\n",
    "                                   trans_freq=0.5) for sample_period in [3, 5, 10]\n",
    "}\n",
    "\n",
    "test_examples = {\n",
    "    sample_period: create_examples(sample_period=sample_period,\n",
    "                                   n_target=1,\n",
    "                                   n_random=3,\n",
    "                                   text_df=test_period_df,\n",
    "                                   n_examples=1000,\n",
    "                                   trans_freq=0.1) for sample_period in [3, 5, 10]\n",
    "}\n",
    "\n",
    "\n",
    "print('Number of train/test examples for 3 trailing days: {}/{}'.format(len(train_examples[3]), len(test_examples[3])))\n",
    "print('Number of train/test examples for 5 trailing days: {}/{}'.format(len(train_examples[5]), len(test_examples[5])))\n",
    "print('Number of train/test examples for 10 trailing days: {}/{}'.format(len(train_examples[10]), len(test_examples[10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percet Transitions 3 Trailing Days Train: 56.43%\n",
      "Percet Transitions 3 Trailing Days Test: 5.88%\n",
      "Percet Transitions 5 Trailing Days Train: 47.42%\n",
      "Percet Transitions 5 Trailing Days Test: 6.15%\n",
      "Percet Transitions 10 Trailing Days Train: 44.44%\n",
      "Percet Transitions 10 Trailing Days Test: 7.22%\n"
     ]
    }
   ],
   "source": [
    "def get_true_frac(example_list):\n",
    "    n_example = len(example_list)\n",
    "    n_trans = sum([example['transition'] for example in example_list])\n",
    "    n_nontrans = n_example - n_trans\n",
    "    return float(n_trans)/float(n_example)\n",
    "    \n",
    "for i in [3, 5, 10]:\n",
    "    print('Percet Transitions {} Trailing Days Train: {:.2%}'.format(i, get_true_frac(train_examples[i])))\n",
    "    print('Percet Transitions {} Trailing Days Test: {:.2%}'.format(i, get_true_frac(test_examples[i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [3, 5, 10]:\n",
    "    with open(\"../../data/heirarchical/processed data/{}-day-example-map-train.json\".format(i), \"w\") as json_file:\n",
    "        json.dump(train_examples[i], json_file)\n",
    "    with open(\"../../data/heirarchical/processed data/{}-day-example-map-test.json\".format(i), \"w\") as json_file:\n",
    "        json.dump(test_examples[i], json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples[3][0]['target_samples'] in train_X_df['text_hash'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14394462476021379538"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples[3][0]['target_samples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
